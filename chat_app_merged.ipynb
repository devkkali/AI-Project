{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pickle\n",
    "import numpy as np\n",
    "import tkinter as tk\n",
    "from tkinter import ttk\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from transformers import TFGPT2LMHeadModel, GPT2LMHeadModel, GPT2Tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = 'dataset/'\n",
    "yoda_file = 'yoda-corpus.csv'\n",
    "model_path = 'model/trained_model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_message():\n",
    "    message = input_field.get()\n",
    "    selected_option = radio_var.get()\n",
    "\n",
    "    if selected_option == 1:\n",
    "        output_text.insert(tk.END, \"user: \" + message + \"\\n\")\n",
    "        response = getResponsefromGan(message)\n",
    "        output_text.insert(tk.END, \"GAN: \" + response + \"\\n\")\n",
    "    elif selected_option == 2:\n",
    "        output_text.insert(tk.END, \"user: \" + message + \"\\n\")\n",
    "        response = getResponsefromPretrainedModel(message,tokenizer_untrained,model_untrained)\n",
    "        output_text.insert(tk.END, \"Untrained Model:\" + response + \"\\n\")\n",
    "    elif selected_option == 3:\n",
    "        output_text.insert(tk.END, \"user: \" + message + \"\\n\")\n",
    "        response = getResponsefromPretrainedModel(message,tokenizer_trained,model_trained)\n",
    "        output_text.insert(tk.END, \"Pretrained Model (Fine Tuned): \" + response + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getResponsefromPretrainedModel(message,tokenizer,model):\n",
    "    input_ids = tokenizer.encode(message, add_special_tokens=True, return_tensors=\"pt\")\n",
    "    attention_mask = torch.ones_like(input_ids)\n",
    "    output = model.generate(input_ids, attention_mask=attention_mask, max_length=30, num_return_sequences=1)\n",
    "    response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getResponsefromGan(message):\n",
    "    tokens = preprocess_input(message)\n",
    "    response = generate_response(tokens)\n",
    "    response = postprocess_response(response)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model/gan_model\n",
      "model_3\n",
      "model/gan_model\\model_3\n",
      "model/gan_model\\model_3\n"
     ]
    }
   ],
   "source": [
    "model_path = \"model/trained_model\"\n",
    "gan_model_dir = \"model/gan_model\"\n",
    "gan_folder_name = \"model_3\"\n",
    "print(gan_model_dir)\n",
    "print(gan_folder_name)\n",
    "gan_full_path = os.path.join(gan_model_dir, gan_folder_name)\n",
    "print(gan_full_path)\n",
    "\n",
    "tokenizer_trained = GPT2Tokenizer.from_pretrained(model_path)\n",
    "tokenizer_untrained = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "model_trained = GPT2LMHeadModel.from_pretrained(model_path)\n",
    "model_untrained = GPT2LMHeadModel.from_pretrained(\"gpt2\",pad_token_id=tokenizer_untrained.eos_token_id)\n",
    "\n",
    "model_trained.eval()\n",
    "model_untrained.eval()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Load the compiled Generator model\n",
    "print(gan_full_path)\n",
    "model_gan = load_model(os.path.join(gan_full_path, \"generator_model_compiled.h5\"))\n",
    "\n",
    "# Load the Tokenizer\n",
    "with open(os.path.join(gan_full_path, \"tokenizer.pkl\"), \"rb\") as tokenizer_file:\n",
    "    tokenizer_gan_data = pickle.load(tokenizer_file)\n",
    "\n",
    "tokenizer_gan = tokenizer_gan_data['tokenizer']\n",
    "max_sequence_length_gan = tokenizer_gan_data['max_sequence_length']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_input(user_input):\n",
    "    # Tokenize the user input\n",
    "    tokens = user_input.strip().split()\n",
    "\n",
    "    # Convert tokens to lowercase\n",
    "    tokens = [token.lower() for token in tokens]\n",
    "\n",
    "    # Return the preprocessed input\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def generate_response(input_tokens, generator_model = model_gan ):\n",
    "    # Convert input tokens to numerical representation\n",
    "    input_sequence = tokenizer_gan.texts_to_sequences([input_tokens])\n",
    "    input_sequence = pad_sequences(input_sequence, maxlen=max_sequence_length_gan-1)\n",
    "    # Generate response using the generator model\n",
    "    generated_sequence = model_gan.predict(input_sequence)\n",
    "\n",
    "    # Convert numerical representation back to tokens\n",
    "    # generated_tokens = tokenizer.sequences_to_texts(generated_sequence)[0].split()\n",
    "\n",
    "    generated_tokens = [tokenizer_gan.index_word.get(index, \"\") for index in np.argmax(generated_sequence, axis=-1)[0]]\n",
    "    generated_tokens = [token for token in generated_tokens if token]\n",
    "\n",
    "    # print('generated_sequence function:',generated_tokens)\n",
    "\n",
    "    # Return the generated response tokens\n",
    "    return generated_tokens\n",
    "\n",
    "\n",
    "def postprocess_response(response_tokens):\n",
    "    # Convert tokens to string\n",
    "    response_text = ' '.join(response_tokens)\n",
    "\n",
    "    # Return the postprocessed response\n",
    "    return response_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 1s/step\n"
     ]
    }
   ],
   "source": [
    "# Create the main window\n",
    "window = tk.Tk()\n",
    "window.title(\"Simple UI Example\")\n",
    "\n",
    "# Calculate the center position of the window\n",
    "window_width = 400\n",
    "window_height = 300\n",
    "screen_width = window.winfo_screenwidth()\n",
    "screen_height = window.winfo_screenheight()\n",
    "x_coordinate = int((screen_width/2) - (window_width/2))\n",
    "y_coordinate = int((screen_height/2) - (window_height/2))\n",
    "\n",
    "# Set the window position\n",
    "window.geometry(f\"{window_width}x{window_height}+{x_coordinate}+{y_coordinate}\")\n",
    "\n",
    "# Create a division using LabelFrame\n",
    "form_frame = ttk.LabelFrame(window, text=\"Form\", style=\"Custom.TLabelframe\")\n",
    "form_frame.pack(padx=10, pady=10)\n",
    "\n",
    "# Custom style for LabelFrame\n",
    "style = ttk.Style()\n",
    "style.configure(\"Custom.TLabelframe\", background=\"white\")\n",
    "\n",
    "# Create the \"Input\" label\n",
    "input_label = ttk.Label(form_frame, text=\"User:\")\n",
    "input_label.grid(row=0, column=0, padx=10, pady=(0, 5), sticky=\"w\")\n",
    "\n",
    "# Create the input field\n",
    "input_field = ttk.Entry(form_frame, font=('Arial', 12))\n",
    "input_field.grid(row=1, column=0, columnspan=3, padx=10, pady=(0, 10), sticky=\"w\")\n",
    "\n",
    "# Create the radio buttons\n",
    "radio_var = tk.IntVar()\n",
    "\n",
    "radio_frame = ttk.Frame(form_frame)\n",
    "radio_frame.grid(row=2, column=0, padx=10, pady=10, columnspan=3)\n",
    "\n",
    "radio_btn1 = ttk.Radiobutton(radio_frame, text=\"GAN\", variable=radio_var, value=1)\n",
    "radio_btn1.grid(row=0, column=0, padx=5)\n",
    "\n",
    "radio_btn2 = ttk.Radiobutton(radio_frame, text=\"Untrained Model\", variable=radio_var, value=2)\n",
    "radio_btn2.grid(row=0, column=1, padx=5)\n",
    "\n",
    "radio_btn3 = ttk.Radiobutton(radio_frame, text=\"Pretrained Model (Fine Tuned)\", variable=radio_var, value=3)\n",
    "radio_btn3.grid(row=0, column=2, padx=5)\n",
    "\n",
    "# Create the send button\n",
    "send_button = ttk.Button(form_frame, text=\"Send\", command=send_message)\n",
    "send_button.grid(row=3, column=0, padx=10, pady=10)\n",
    "\n",
    "# Create the output field\n",
    "output_label = ttk.Label(form_frame, text=\"Output:\")\n",
    "output_label.grid(row=4, column=0, padx=10, pady=(5, 0), sticky=\"w\")\n",
    "\n",
    "output_text = tk.Text(form_frame, height=5, font=('Arial', 12), bd=1, relief=tk.SOLID)\n",
    "output_text.grid(row=5, column=0, columnspan=3, padx=10, pady=(0, 10), sticky=\"nsew\")\n",
    "\n",
    "# Set the input field width to match the output field\n",
    "input_field.config(width=output_text[\"width\"])\n",
    "\n",
    "# Configure grid weights\n",
    "form_frame.columnconfigure(0, weight=1)\n",
    "form_frame.rowconfigure(5, weight=1)\n",
    "\n",
    "# Start the main loop\n",
    "window.mainloop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
