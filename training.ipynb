{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***Install transformer***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install transformers\n",
    "# %pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import TFGPT2LMHeadModel, GPT2LMHeadModel, GPT2Tokenizer\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "log_dir = \"./logs\"  # Specify the directory where you want to save the logs\n",
    "writer = SummaryWriter(log_dir)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***file path***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = 'dataset/'\n",
    "yoda_file = 'yoda-corpus.csv'\n",
    "model_path = 'model/trained_model1'\n",
    "# path = '/content/drive/My Drive/Colab Notebooks/dataset/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(dataset_path+yoda_file)\n",
    "yoda_dialouges=df.loc[df['character']== 'YODA','text']\n",
    "dialouge_array = np.array(yoda_dialouges)\n",
    "dialouge_array = dialouge_array[:5]\n",
    "len(dialouge_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The very Republic is threatened, if involved the Sith are.'\n",
      " 'Hard to see, the dark side is. Discover who this assassin is, we must.'\n",
      " 'With this Naboo queen you must stay, Qui-Gon. Protect her.'\n",
      " 'May the Force be with you.'\n",
      " \"(Cont'd) Master Qui-Gon more to say have you?\"]\n"
     ]
    }
   ],
   "source": [
    "print(dialouge_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model():\n",
    "        # Load the pre-trained GPT2 model and tokenizer\n",
    "        # model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "        tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "        model = GPT2LMHeadModel.from_pretrained(\"gpt2\",pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "        # Create a generation configuration based on the model's configuration\n",
    "        # generation_config = Gpt2GenerationConfig.from_pretrained(\"gpt2\")\n",
    "        # generation_config.pad_token_id = generation_config.eos_token_id\n",
    "\n",
    "        # Define your input data\n",
    "        # input_texts = [\n",
    "        #     \"Hello!\",\n",
    "        #     \"How are you?\",\n",
    "        #     \"What is your name?\",\n",
    "        #     \"Tell me a joke.\",\n",
    "        #     \"Goodbye!\",\n",
    "        #     \"Roshan is good boy\",\n",
    "        #     \"i hope you will get well soon\"\n",
    "        # ]\n",
    "        \n",
    "        df = pd.read_csv('dataset/TweetsElonMusk.csv')\n",
    "\n",
    "        # Filter tweets by language (English)\n",
    "        df = df[df['language'] == 'en']\n",
    "\n",
    "        # Remove words starting with \"@\"\n",
    "        cleaned_dialogues = []\n",
    "\n",
    "        for tweet in df['tweet']:\n",
    "            dialogue = re.sub(r'@\\w+\\s?', '', tweet)\n",
    "            cleaned_dialogues.append(dialogue)\n",
    "\n",
    "        # Tokenize the input texts\n",
    "        tokenized_texts = [tokenizer.encode(text, add_special_tokens=True) for text in cleaned_dialogues[:100]]\n",
    "        # tokenized_texts = [tokenizer.encode(text, add_special_tokens=True) for text in dialouge_array ]\n",
    "\n",
    "        # Set the maximum sequence length\n",
    "        max_seq_length = 32\n",
    "\n",
    "        # Truncate or split the input sequences if they exceed the maximum sequence length\n",
    "        input_ids = []\n",
    "        attention_masks = []\n",
    "        for tokens in tokenized_texts:\n",
    "            if len(tokens) > max_seq_length:\n",
    "                tokens = tokens[:max_seq_length]\n",
    "            input_ids.append(tokens)\n",
    "            attention_masks.append([1] * len(tokens))\n",
    "\n",
    "        # Pad the sequences within a batch\n",
    "        input_ids = pad_sequence([torch.tensor(ids) for ids in input_ids], batch_first=True)\n",
    "        attention_masks = pad_sequence([torch.tensor(ids) for ids in attention_masks], batch_first=True)\n",
    "\n",
    "        # Define the batch size\n",
    "        batch_size = 3\n",
    "\n",
    "        # Create batches of the input sequences\n",
    "        batches = []\n",
    "        for i in range(0, len(input_ids), batch_size):\n",
    "            batch_input_ids = input_ids[i:i+batch_size]\n",
    "            batch_attention_masks = attention_masks[i:i+batch_size]\n",
    "            batch = {\"input_ids\": batch_input_ids, \"attention_mask\": batch_attention_masks}\n",
    "            batches.append(batch)\n",
    "\n",
    "        # Set the model to training mode\n",
    "        model.train()\n",
    "\n",
    "        # Define the optimizer and learning rate\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "\n",
    "        # Start TensorBoard using subprocess\n",
    "        tensorboard_process = subprocess.Popen([\"tensorboard\", \"--logdir\", log_dir])\n",
    "\n",
    "        # Training loop\n",
    "        epochs = 3\n",
    "        for epoch in range(epochs):\n",
    "            # Shuffle the batches\n",
    "            shuffled_batches = torch.randperm(len(batches))\n",
    "\n",
    "            # Iterate over the shuffled batches\n",
    "            for batch_index in shuffled_batches:\n",
    "                batch = batches[batch_index]\n",
    "\n",
    "                # Clear gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # Forward pass\n",
    "                outputs = model(input_ids=batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"], labels=batch[\"input_ids\"])\n",
    "                loss = outputs.loss\n",
    "\n",
    "                # Backward pass and optimization\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                # Print training progress\n",
    "                print(f\"Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}\")\n",
    "                # Write loss to TensorBoard\n",
    "                writer.add_scalar(\"Training Loss\", loss.item(), epoch)\n",
    "        writer.close()\n",
    "        print(f\"tensorboard --logdir={log_dir}\")\n",
    "\n",
    "        # Save the trained model and tokenizer\n",
    "        # model.save_pretrained(\"trained_model\")\n",
    "        # tokenizer.save_pretrained(\"trained_model\")\n",
    "        # generation_config.save_pretrained(\"trained_model\")\n",
    "        global modal\n",
    "        global tokenizar\n",
    "        modal= model\n",
    "        tokenizar=tokenizer\n",
    "        # model.save_pretrained('/content/drive/My Drive/Colab Notebooks/model/trained_model')\n",
    "        # tokenizer.save_pretrained('/content/drive/My Drive/Colab Notebooks/model/trained_model')\n",
    "        model.save_pretrained(model_path)\n",
    "        tokenizer.save_pretrained(model_path)\n",
    "        # generation_config.save_pretrained('/content/drive/My Drive/Colab Notebooks/model2/trained_model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/3], Loss: 6.6957\n",
      "Epoch [1/3], Loss: 6.8936\n",
      "Epoch [1/3], Loss: 6.4712\n",
      "Epoch [1/3], Loss: 5.9778\n",
      "Epoch [1/3], Loss: 5.8733\n",
      "Epoch [1/3], Loss: 5.3025\n",
      "Epoch [1/3], Loss: 5.5032\n",
      "Epoch [1/3], Loss: 5.6692\n",
      "Epoch [1/3], Loss: 5.2720\n",
      "Epoch [1/3], Loss: 4.3009\n",
      "Epoch [1/3], Loss: 4.9333\n",
      "Epoch [1/3], Loss: 4.4548\n",
      "Epoch [1/3], Loss: 4.6386\n",
      "Epoch [1/3], Loss: 3.1584\n",
      "Epoch [1/3], Loss: 4.8949\n",
      "Epoch [1/3], Loss: 4.3700\n",
      "Epoch [1/3], Loss: 4.0593\n",
      "Epoch [1/3], Loss: 4.0758\n",
      "Epoch [1/3], Loss: 4.0405\n",
      "Epoch [1/3], Loss: 3.7532\n",
      "Epoch [1/3], Loss: 3.6380\n",
      "Epoch [1/3], Loss: 2.6116\n",
      "Epoch [1/3], Loss: 2.5109\n",
      "Epoch [1/3], Loss: 3.0051\n",
      "Epoch [1/3], Loss: 1.9036\n",
      "Epoch [1/3], Loss: 4.1905\n",
      "Epoch [1/3], Loss: 3.3620\n",
      "Epoch [1/3], Loss: 3.8097\n",
      "Epoch [1/3], Loss: 3.0238\n",
      "Epoch [1/3], Loss: 3.3307\n",
      "Epoch [1/3], Loss: 3.3281\n",
      "Epoch [1/3], Loss: 3.2004\n",
      "Epoch [1/3], Loss: 3.5553\n",
      "Epoch [1/3], Loss: 2.5285\n",
      "Epoch [2/3], Loss: 2.7659\n",
      "Epoch [2/3], Loss: 0.9612\n",
      "Epoch [2/3], Loss: 3.1471\n",
      "Epoch [2/3], Loss: 2.8837\n",
      "Epoch [2/3], Loss: 3.5201\n",
      "Epoch [2/3], Loss: 2.1670\n",
      "Epoch [2/3], Loss: 4.9476\n",
      "Epoch [2/3], Loss: 3.3458\n",
      "Epoch [2/3], Loss: 2.3961\n",
      "Epoch [2/3], Loss: 2.8193\n",
      "Epoch [2/3], Loss: 3.3966\n",
      "Epoch [2/3], Loss: 2.8820\n",
      "Epoch [2/3], Loss: 1.9029\n",
      "Epoch [2/3], Loss: 1.3660\n",
      "Epoch [2/3], Loss: 1.7520\n",
      "Epoch [2/3], Loss: 2.6261\n",
      "Epoch [2/3], Loss: 2.3923\n",
      "Epoch [2/3], Loss: 2.8408\n",
      "Epoch [2/3], Loss: 3.0084\n",
      "Epoch [2/3], Loss: 3.2593\n",
      "Epoch [2/3], Loss: 2.9279\n",
      "Epoch [2/3], Loss: 1.8435\n",
      "Epoch [2/3], Loss: 2.8747\n",
      "Epoch [2/3], Loss: 2.7419\n",
      "Epoch [2/3], Loss: 3.2950\n",
      "Epoch [2/3], Loss: 2.3140\n",
      "Epoch [2/3], Loss: 3.4323\n",
      "Epoch [2/3], Loss: 3.2296\n",
      "Epoch [2/3], Loss: 2.8542\n",
      "Epoch [2/3], Loss: 1.4682\n",
      "Epoch [2/3], Loss: 4.1917\n",
      "Epoch [2/3], Loss: 3.0338\n",
      "Epoch [2/3], Loss: 0.9798\n",
      "Epoch [2/3], Loss: 4.5533\n",
      "Epoch [3/3], Loss: 3.3552\n",
      "Epoch [3/3], Loss: 2.9231\n",
      "Epoch [3/3], Loss: 2.9856\n",
      "Epoch [3/3], Loss: 2.7527\n",
      "Epoch [3/3], Loss: 2.4653\n",
      "Epoch [3/3], Loss: 2.3845\n",
      "Epoch [3/3], Loss: 3.8600\n",
      "Epoch [3/3], Loss: 2.3417\n",
      "Epoch [3/3], Loss: 2.7453\n",
      "Epoch [3/3], Loss: 2.6834\n",
      "Epoch [3/3], Loss: 1.9121\n",
      "Epoch [3/3], Loss: 2.4340\n",
      "Epoch [3/3], Loss: 2.3585\n",
      "Epoch [3/3], Loss: 1.4236\n",
      "Epoch [3/3], Loss: 2.0539\n",
      "Epoch [3/3], Loss: 0.7369\n",
      "Epoch [3/3], Loss: 1.7631\n",
      "Epoch [3/3], Loss: 2.8018\n",
      "Epoch [3/3], Loss: 0.8478\n",
      "Epoch [3/3], Loss: 2.3410\n",
      "Epoch [3/3], Loss: 2.9856\n",
      "Epoch [3/3], Loss: 3.6621\n",
      "Epoch [3/3], Loss: 3.3132\n",
      "Epoch [3/3], Loss: 1.8206\n",
      "Epoch [3/3], Loss: 2.9420\n",
      "Epoch [3/3], Loss: 2.6838\n",
      "Epoch [3/3], Loss: 2.8268\n",
      "Epoch [3/3], Loss: 1.2658\n",
      "Epoch [3/3], Loss: 3.6549\n",
      "Epoch [3/3], Loss: 3.8551\n",
      "Epoch [3/3], Loss: 0.9964\n",
      "Epoch [3/3], Loss: 2.1570\n",
      "Epoch [3/3], Loss: 3.2387\n",
      "Epoch [3/3], Loss: 4.7280\n",
      "tensorboard --logdir=./logs\n"
     ]
    }
   ],
   "source": [
    "train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
